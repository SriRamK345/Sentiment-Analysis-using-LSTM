{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyO00l2gVfHgRQESazzH1XCy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SriRamK345/Sentiment-Analysis-using-LSTM/blob/main/Sentiment_Analysis_using_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sDkaKngwUyGZ"
      },
      "outputs": [],
      "source": [
        "! unzip twitter_training.csv.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Necessary Library"
      ],
      "metadata": {
        "id": "mWmDycKCHAhf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional, BatchNormalization\n",
        "from tensorflow.keras.regularizers import l2\n",
        "import spacy\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "o2XevQkvGxmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reading The Data"
      ],
      "metadata": {
        "id": "oG1YWw8_HI3T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = pd.read_csv(\"/content/twitter_training.csv\")\n",
        "df_test = pd.read_csv(\"//content/twitter_validation.csv\")"
      ],
      "metadata": {
        "id": "qq25q1dqHJjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explore The Data"
      ],
      "metadata": {
        "id": "vO_gmpuhHvFH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.head()"
      ],
      "metadata": {
        "id": "1zJ1CveQHvju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test.head()"
      ],
      "metadata": {
        "id": "77frwWTeHudX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adding headers"
      ],
      "metadata": {
        "id": "FshkvczpIBWv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_test.columns = ['Header1', 'company','labels','text']\n",
        "df_train.columns = ['Header1', 'company','labels','text']"
      ],
      "metadata": {
        "id": "LVrFLl3hIKj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.head()"
      ],
      "metadata": {
        "id": "GE-JX1pJIQFd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test.head()"
      ],
      "metadata": {
        "id": "GOtDXwDvIQIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.shape, df_test.shape"
      ],
      "metadata": {
        "id": "zuJh1OEzIQM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.info()\n",
        "print(\"\\n\")\n",
        "df_train.info()"
      ],
      "metadata": {
        "id": "qeU39ce9IQPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.duplicated().sum()"
      ],
      "metadata": {
        "id": "-PuzdJ_-IQSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test.duplicated().sum()"
      ],
      "metadata": {
        "id": "E6fVOOLnIQU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.isnull().sum()"
      ],
      "metadata": {
        "id": "XLwD9qWAI10J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test.isnull().sum()"
      ],
      "metadata": {
        "id": "eKP6frl5I13H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cleaning the Data"
      ],
      "metadata": {
        "id": "S9D8HZZ3JDRr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "R02PF1IbJEHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.isnull().sum()"
      ],
      "metadata": {
        "id": "dKDE3IQxJEu5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.drop_duplicates(inplace=True)"
      ],
      "metadata": {
        "id": "JvNVFMW6JEyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.duplicated().sum()"
      ],
      "metadata": {
        "id": "tAsUjvc6I150"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Drop Uneeded Columns**"
      ],
      "metadata": {
        "id": "uMIsylPCJNQd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.drop(columns=['Header1', 'company'], inplace=True)"
      ],
      "metadata": {
        "id": "UPP07OXdJTPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test.drop(columns=['Header1', 'company'], inplace=True)"
      ],
      "metadata": {
        "id": "WoDDXoxrJU82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train"
      ],
      "metadata": {
        "id": "0ozmtYXyJYsK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test"
      ],
      "metadata": {
        "id": "nWup60naJaeU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing the Data"
      ],
      "metadata": {
        "id": "Ea7zRMjlJe33"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "1ddq87QRJfZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words(\"english\"))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "number_pattern = re.compile(r\"\\d+\")\n",
        "\n",
        "def preprocess_test(text):\n",
        "  text = number_pattern.sub(\"\", text)  # Remove numbers\n",
        "  text = text.translate(str.maketrans(\"\", \"\", string.punctuation)).lower()  # Remove punctuation and lowercase\n",
        "  text = [lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words] # Lemmatization & stop word removal\n",
        "  return \" \".join(text)"
      ],
      "metadata": {
        "id": "2Vq_MOOhJdSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train['train_text'] = df_train['text'].apply(preprocess_test)\n",
        "df_test['test_text'] = df_test['text'].apply(preprocess_test)"
      ],
      "metadata": {
        "id": "5uzPDlxCJdz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split the Data"
      ],
      "metadata": {
        "id": "knignASwLp1z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate features and labels for training data\n",
        "train_texts = df_train['train_text'].values\n",
        "train_labels = df_train['labels'].values\n",
        "\n",
        "# Separate features and labels for test data\n",
        "test_texts = df_test['test_text'].values\n",
        "test_labels = df_test['labels'].values"
      ],
      "metadata": {
        "id": "7Sjx7Ua3LXit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the label encoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Fit and transform the labels\n",
        "train_labels_encoded = label_encoder.fit_transform(train_labels)\n",
        "test_labels_encoded = label_encoder.transform(test_labels)"
      ],
      "metadata": {
        "id": "yF_UAy7SLzc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the tokenizer\n",
        "tokenizer = Tokenizer()  # You can adjust the number of words\n",
        "\n",
        "# Fit the tokenizer on the training texts\n",
        "tokenizer.fit_on_texts(train_texts)\n",
        "tokenizer.fit_on_texts(test_texts)"
      ],
      "metadata": {
        "id": "RANv_FYlLLZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert texts to sequences\n",
        "train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
        "test_sequences = tokenizer.texts_to_sequences(test_texts)"
      ],
      "metadata": {
        "id": "Qhq4_YL0MJMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Maximum Length in X_train_sequences\n",
        "maxlen = max(len(tokens) for tokens in train_sequences)\n",
        "print(\"Maximum sequence length (maxlen):\", maxlen)"
      ],
      "metadata": {
        "id": "xaAHOI9iMVMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_padded = pad_sequences(train_sequences, maxlen=maxlen, padding='post')\n",
        "test_padded = pad_sequences(test_sequences, maxlen=maxlen, padding='post')"
      ],
      "metadata": {
        "id": "WsqmpbXTNJgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Embedding Input Size\n",
        "input_size = np.max(train_padded) + 1\n",
        "input_size"
      ],
      "metadata": {
        "id": "upwcJW1jNME3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building the Model"
      ],
      "metadata": {
        "id": "dIKVApkvNQ3U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "model = Sequential()\n",
        "\n",
        "# Add an embedding layer\n",
        "model.add(Embedding(input_dim=input_size+2, output_dim=100, input_shape=(maxlen,)))\n",
        "\n",
        "# Add a bidirectional LSTM layer with 128 units\n",
        "model.add(Bidirectional(LSTM(128, kernel_regularizer=l2(0.1), return_sequences=True, recurrent_regularizer=l2(0.1))))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Add another LSTM layer\n",
        "model.add(Bidirectional(LSTM(64, kernel_regularizer=l2(0.01), recurrent_regularizer=l2(0.01))))\n",
        "# Add batch normalization layer\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Add a dense layer with 64 units and ReLU activation\n",
        "model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.01)))\n",
        "\n",
        "# Add dropout regularization\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Add the output layer with 5 units for 5 labels and softmax activation\n",
        "model.add(Dense(5, activation='softmax'))"
      ],
      "metadata": {
        "id": "XCkUmuslNRQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "Mnw7qodZNh-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "YLOinwzKNnMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the EarlyStopping callback\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor=\"val_loss\",  # Metric to monitor\n",
        "    patience=10,          # Number of epochs to wait before stopping\n",
        "    restore_best_weights=True  # Restore the best model weights\n",
        ")"
      ],
      "metadata": {
        "id": "x3RGjAtuOrBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    train_padded,\n",
        "    train_labels_encoded ,\n",
        "    validation_data=(test_padded,test_labels_encoded),\n",
        "    verbose=1,\n",
        "    callbacks=[early_stopping],\n",
        "    epochs=50,\n",
        "    batch_size=32\n",
        ")"
      ],
      "metadata": {
        "id": "eZjOMLsbNpuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate the model"
      ],
      "metadata": {
        "id": "hNlQ7xYFNvGf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = model.evaluate(test_padded , test_labels_encoded)\n",
        "print(f'Test Loss: {test_loss}')\n",
        "print(f'Test Accuracy: {test_accuracy}')"
      ],
      "metadata": {
        "id": "lW7jZE1LNv57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualize training history"
      ],
      "metadata": {
        "id": "oP8JVX-7NyyV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training & validation accuracy values\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_W-XmephNuQp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}