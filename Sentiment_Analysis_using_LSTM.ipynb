{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPaYcy53MRsDbo1gfuTjAlj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SriRamK345/Sentiment-Analysis-using-LSTM/blob/main/Sentiment_Analysis_using_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sDkaKngwUyGZ"
      },
      "outputs": [],
      "source": [
        "! unzip twitter_training.csv.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Necessary Library"
      ],
      "metadata": {
        "id": "mWmDycKCHAhf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional, BatchNormalization\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.models import load_model\n",
        "import spacy\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "o2XevQkvGxmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reading The Data"
      ],
      "metadata": {
        "id": "oG1YWw8_HI3T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = pd.read_csv(\"/content/twitter_training.csv\")\n",
        "df_test = pd.read_csv(\"//content/twitter_validation.csv\")"
      ],
      "metadata": {
        "id": "qq25q1dqHJjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explore The Data"
      ],
      "metadata": {
        "id": "vO_gmpuhHvFH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.head()"
      ],
      "metadata": {
        "id": "1zJ1CveQHvju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test.head()"
      ],
      "metadata": {
        "id": "77frwWTeHudX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adding headers"
      ],
      "metadata": {
        "id": "FshkvczpIBWv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_test.columns = ['Header1', 'company','labels','text']\n",
        "df_train.columns = ['Header1', 'company','labels','text']"
      ],
      "metadata": {
        "id": "LVrFLl3hIKj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.head()"
      ],
      "metadata": {
        "id": "GE-JX1pJIQFd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test.head()"
      ],
      "metadata": {
        "id": "GOtDXwDvIQIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.shape, df_test.shape"
      ],
      "metadata": {
        "id": "zuJh1OEzIQM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.info()\n",
        "print(\"\\n\")\n",
        "df_train.info()"
      ],
      "metadata": {
        "id": "qeU39ce9IQPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.duplicated().sum()"
      ],
      "metadata": {
        "id": "-PuzdJ_-IQSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test.duplicated().sum()"
      ],
      "metadata": {
        "id": "E6fVOOLnIQU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.isnull().sum()"
      ],
      "metadata": {
        "id": "XLwD9qWAI10J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test.isnull().sum()"
      ],
      "metadata": {
        "id": "eKP6frl5I13H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cleaning the Data"
      ],
      "metadata": {
        "id": "S9D8HZZ3JDRr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "R02PF1IbJEHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.isnull().sum()"
      ],
      "metadata": {
        "id": "dKDE3IQxJEu5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.drop_duplicates(inplace=True)"
      ],
      "metadata": {
        "id": "JvNVFMW6JEyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.duplicated().sum()"
      ],
      "metadata": {
        "id": "tAsUjvc6I150"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Drop Uneeded Columns**"
      ],
      "metadata": {
        "id": "uMIsylPCJNQd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.drop(columns=['Header1', 'company'], inplace=True)"
      ],
      "metadata": {
        "id": "UPP07OXdJTPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test.drop(columns=['Header1', 'company'], inplace=True)"
      ],
      "metadata": {
        "id": "WoDDXoxrJU82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train"
      ],
      "metadata": {
        "id": "0ozmtYXyJYsK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test"
      ],
      "metadata": {
        "id": "nWup60naJaeU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing the Data"
      ],
      "metadata": {
        "id": "Ea7zRMjlJe33"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "1ddq87QRJfZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words(\"english\"))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "number_pattern = re.compile(r\"\\d+\")\n",
        "\n",
        "def preprocess_test(text):\n",
        "  text = number_pattern.sub(\"\", text)  # Remove numbers\n",
        "  text = text.translate(str.maketrans(\"\", \"\", string.punctuation)).lower()  # Remove punctuation and lowercase\n",
        "  text = [lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words] # Lemmatization & stop word removal\n",
        "  return \" \".join(text)"
      ],
      "metadata": {
        "id": "2Vq_MOOhJdSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train['train_text'] = df_train['text'].apply(preprocess_test)\n",
        "df_test['test_text'] = df_test['text'].apply(preprocess_test)"
      ],
      "metadata": {
        "id": "5uzPDlxCJdz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split the Data"
      ],
      "metadata": {
        "id": "knignASwLp1z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate features and labels for training data\n",
        "train_texts = df_train['train_text'].values\n",
        "train_labels = df_train['labels'].values\n",
        "\n",
        "# Separate features and labels for test data\n",
        "test_texts = df_test['test_text'].values\n",
        "test_labels = df_test['labels'].values"
      ],
      "metadata": {
        "id": "7Sjx7Ua3LXit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the label encoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Fit and transform the labels\n",
        "train_labels_encoded = label_encoder.fit_transform(train_labels)\n",
        "test_labels_encoded = label_encoder.transform(test_labels)"
      ],
      "metadata": {
        "id": "yF_UAy7SLzc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the tokenizer\n",
        "tokenizer = Tokenizer()  # You can adjust the number of words\n",
        "\n",
        "# Fit the tokenizer on the training texts\n",
        "tokenizer.fit_on_texts(train_texts)\n",
        "tokenizer.fit_on_texts(test_texts)"
      ],
      "metadata": {
        "id": "RANv_FYlLLZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert texts to sequences\n",
        "train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
        "test_sequences = tokenizer.texts_to_sequences(test_texts)"
      ],
      "metadata": {
        "id": "Qhq4_YL0MJMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Maximum Length in X_train_sequences\n",
        "maxlen = max(len(tokens) for tokens in train_sequences)\n",
        "print(\"Maximum sequence length (maxlen):\", maxlen)"
      ],
      "metadata": {
        "id": "xaAHOI9iMVMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_padded = pad_sequences(train_sequences, maxlen=maxlen, padding='post')\n",
        "test_padded = pad_sequences(test_sequences, maxlen=maxlen, padding='post')"
      ],
      "metadata": {
        "id": "WsqmpbXTNJgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the maximum value from your training and testing data\n",
        "max_index_train = train_padded.max()\n",
        "max_index_test = test_padded.max()\n",
        "max_index = max(max_index_train, max_index_test)\n",
        "\n",
        "# Increase input_size to accommodate the largest index + 1\n",
        "input_size = max_index + 1"
      ],
      "metadata": {
        "id": "kv6dY6tlkdNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building the Model"
      ],
      "metadata": {
        "id": "dIKVApkvNQ3U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "model = Sequential()\n",
        "\n",
        "# Add an embedding layer\n",
        "model.add(Embedding(input_dim=input_size, output_dim=100, input_shape=(maxlen,)))\n",
        "\n",
        "# Add a bidirectional LSTM layer with 128 units\n",
        "model.add(Bidirectional(LSTM(128, kernel_regularizer=l2(0.1), return_sequences=True, recurrent_regularizer=l2(0.1))))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Add another LSTM layer\n",
        "model.add(Bidirectional(LSTM(64, kernel_regularizer=l2(0.01), recurrent_regularizer=l2(0.01))))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Add a dense layer with 64 units and ReLU activation\n",
        "model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.01)))\n",
        "\n",
        "# Add dropout regularization\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Add the output layer with 5 units for 5 labels and softmax activation\n",
        "model.add(Dense(5, activation='softmax'))"
      ],
      "metadata": {
        "id": "Ntvs5VRjmU-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "Mnw7qodZNh-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "YLOinwzKNnMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the EarlyStopping callback\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor=\"val_loss\",  # Metric to monitor\n",
        "    patience=3,          # Number of epochs to wait before stopping\n",
        "    restore_best_weights=True  # Restore the best model weights\n",
        ")"
      ],
      "metadata": {
        "id": "x3RGjAtuOrBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    train_padded,\n",
        "    train_labels_encoded ,\n",
        "    validation_data=(test_padded,test_labels_encoded),\n",
        "    callbacks=[early_stopping],\n",
        "    epochs=50,\n",
        "    batch_size=64\n",
        ")"
      ],
      "metadata": {
        "id": "eZjOMLsbNpuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate the model"
      ],
      "metadata": {
        "id": "hNlQ7xYFNvGf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = model.evaluate(test_padded , test_labels_encoded)\n",
        "print(f'Test Loss: {test_loss}')\n",
        "print(f'Test Accuracy: {test_accuracy}')"
      ],
      "metadata": {
        "id": "lW7jZE1LNv57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification report\n",
        "y_pred = model.predict(test_padded)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "print(classification_report(test_labels_encoded, y_pred_classes))"
      ],
      "metadata": {
        "id": "xYSG3ngeqcBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualize training history"
      ],
      "metadata": {
        "id": "oP8JVX-7NyyV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training & validation accuracy values\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_W-XmephNuQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save the model"
      ],
      "metadata": {
        "id": "n3Av_OMSwze4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('senti_model.keras')"
      ],
      "metadata": {
        "id": "XlyvxUzvsFDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the trained model\n"
      ],
      "metadata": {
        "id": "o522MJkYw2P4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model = load_model('/content/senti_model.keras')"
      ],
      "metadata": {
        "id": "JL-A_rSksH-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "def predict_sentiment(text, model):\n",
        "    \"\"\"\n",
        "    Predicts the sentiment of a given text using a trained model.\n",
        "\n",
        "    Args:\n",
        "        text (str or list): The text to predict the sentiment for. Can be a single string or a list of strings.\n",
        "        model: The trained Keras model.\n",
        "\n",
        "    Returns:\n",
        "        str or list: The predicted sentiment label(s). If input is a string, returns a string. If input is a list, returns a list.\n",
        "    \"\"\"\n",
        "\n",
        "    if isinstance(text, str):\n",
        "        # If text is a single string, process as before\n",
        "        preprocessed_text = preprocess_test(text)\n",
        "        test_sequence = tokenizer.texts_to_sequences([preprocessed_text])\n",
        "        test_padded = pad_sequences(test_sequence, maxlen=maxlen, padding='post')\n",
        "        predict = model.predict(test_padded)\n",
        "        predicted_label = np.argmax(predict)\n",
        "        original_label = label_encoder.inverse_transform([predicted_label])\n",
        "        return original_label[0]\n",
        "    elif isinstance(text, list):\n",
        "        # If text is a list of strings, process each string individually\n",
        "        return [predict_sentiment(t, model) for t in text]\n",
        "    else:\n",
        "        raise TypeError(\"Input 'text' must be a string or a list of strings.\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "5VHsCRcFxP_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_text = input(\"Enter a text: \")\n",
        "predicted_sentiment = predict_sentiment(user_text, loaded_model)\n",
        "print(predicted_sentiment)"
      ],
      "metadata": {
        "id": "lGKVYvvMxz9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test['predicted_sentiment'] = df_test['test_text'].apply(lambda x: predict_sentiment(x, loaded_model))"
      ],
      "metadata": {
        "id": "4pce4QIwx3fU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test.iloc[6:20][[\"labels\",\"predicted_sentiment\"]]"
      ],
      "metadata": {
        "id": "IouxavMizXJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s25urscY4RCt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}